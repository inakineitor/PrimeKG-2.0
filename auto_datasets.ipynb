{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "# Directory for the datasets folder in which the program will run\n",
    "\n",
    "directory = '/Users/vir/Testing/datasets'\n",
    "\n",
    "os.chdir(directory)\n",
    "\n",
    "# Dictionary of urls and filepaths for data \n",
    "\n",
    "data_dict = {\"https://www.genenames.org/cgi-bin/download/custom?col=gd_app_sym&col=gd_app_name&col=gd_pub_acc_ids&col=gd_pub_refseq_ids&col=gd_pub_eg_id&col=md_eg_id&col=md_prot_id&col=md_mim_id&status=Approved&hgnc_dbtag=on&order_by=gd_app_sym_sort&format=text&submit=submit\": 'data/vocab/gene_names.csv'}\n",
    "data_dict['https://www.bgee.org/ftp/current/download/calls/expr_calls/Homo_sapiens_expr_advanced.tsv.gz'] = 'data/bgee/Homo_sapiens_expr_advanced.tsv.gz'\n",
    "data_dict['https://ctdbase.org/reports/CTD_exposure_events.csv.gz'] = 'data/ctd/CTD_exposure_events.csv.gz'\n",
    "data_dict['https://www.disgenet.org/static/disgenet_ap1/files/downloads/curated_gene_disease_associations.tsv.gz'] = 'data/disgenet/curated_gene_disease_associations.tsv.gz'\n",
    "data_dict['https://ftp.ncbi.nlm.nih.gov/gene/DATA/gene2go.gz'] = 'data/ncbigene/gene2go.gz'\n",
    "data_dict['http://purl.obolibrary.org/obo/go/go-basic.obo'] = 'data/go/go-basic.obo'\n",
    "data_dict['http://purl.obolibrary.org/obo/hp.obo'] = 'data/hpo/hp.obo'\n",
    "data_dict['http://purl.obolibrary.org/obo/hp/hpoa/phenotype.hpoa'] = 'data/hpo/phenotype.hpoa'\n",
    "data_dict['http://purl.obolibrary.org/obo/MONDO.obo'] = 'data/mondo/mondo.obo'\n",
    "data_dict['https://reactome.org/download/current/ReactomePathways.txt,https://reactome.org/download/current/ReactomePathwaysRelation.txt,https://reactome.org/download/current/NCBI2Reactome.txt'] = 'data/reactome/ReactomePathways.txt,data/reactome/ReactomePathwaysRelation.txt,data/reactome/NCBI2Reactome.txt'\n",
    "data_dict['http://sideeffects.embl.de/media/download/meddra_all_se.tsv.gz,http://sideeffects.embl.de/media/download/drug_atc.tsv'] = 'data/sider/meddra_all_se.tsv.gz,data/sider/drug_atc.tsv'\n",
    "data_dict['http://purl.obolibrary.org/obo/uberon/ext.obo'] = 'data/uberon/ext.obo'\n",
    "data_dict['https://unmtid-shinyapps.net/download/drugcentral.dump.05102023.sql.gz'] = 'data/drugcentral/drugcentral.dump.05102023.sql.gz'\n",
    "data_keys = list(data_dict.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   343    0   343    0     0   2074      0 --:--:-- --:--:-- --:--:--  2241\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 20.8M  100 20.8M    0     0  12.0M      0  0:00:01  0:00:01 --:--:-- 21.8M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Size of data/uberon/uberon_is_a.csv in Bytes is 166711.\n",
      "File Size of data/uberon/uberon_rels.csv in Bytes is 212438.\n",
      "File Size of data/uberon/uberon_terms.csv in Bytes is 2588863.\n"
     ]
    }
   ],
   "source": [
    "def initialize(directory):\n",
    "    os.chdir(directory)\n",
    "    if 'datasets' not in directory:\n",
    "        print('Make sure you have put in the proper directory for the program to run.')\n",
    "    subprocess.run(['mkdir', 'data'])\n",
    "    subprocess.run('mkdir data/bgee data/ctd data/disgenet data/drugbank data/vocab data/drugcentral data/ncbigene data/go data/hpo data/mondo data/reactome data/sider data/uberon data/umls'.split(' '))\n",
    "\n",
    "def gene_names(url, filepath):\n",
    "    os.system(f'curl {url} -o {filepath}')\n",
    "\n",
    "\n",
    "def bgee(url, filepath):\n",
    "    directory = os.getcwd()\n",
    "\n",
    "    # Downloading data\n",
    "    os.system(f'curl {url} -o {filepath}')\n",
    "    os.system(f'gunzip {filepath}')\n",
    "\n",
    "    # Validating data size \n",
    "    if os.path.getsize(filepath) < 46804992:\n",
    "        print(f'Warning: {filepath} file may have not downloaded fully') \n",
    "\n",
    "    # Running processing script\n",
    "    os.chdir(directory + '/processing_scripts')\n",
    "    os.system('python bgee.py')\n",
    "    os.chdir(directory)\n",
    "\n",
    "    # Validating processing script output\n",
    "    try:\n",
    "        file_size = os.path.getsize('data/bgee/anatomy_gene.csv')\n",
    "        print(f\"File Size of data/bgee/anatomy_gene.csv in Bytes is {file_size}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Check bgee.py to make sure data is being appropriatly processed and saved.\")\n",
    "\n",
    "\n",
    "def ctd(url, filepath):\n",
    "    directory = os.getcwd()\n",
    "\n",
    "    # Downloading data\n",
    "    os.system(f'curl {url} -o {filepath}')\n",
    "    os.system(f'gunzip {filepath}')\n",
    "\n",
    "    # Validating data size \n",
    "    if os.path.getsize(filepath) < 2085711:\n",
    "        print(f'Warning: {filepath} file may have not downloaded fully')\n",
    "    \n",
    "    # Running processing script\n",
    "    os.chdir(directory + '/processing_scripts')\n",
    "    os.system('python ctd.py')\n",
    "    os.chdir(directory)\n",
    "\n",
    "    # Validating processing script output\n",
    "    try:\n",
    "        file_size = os.path.getsize('data/ctd/CTD_exposure_events.csv')\n",
    "        print(f\"File Size of data/ctd/CTD_exposure_events.csv in Bytes is {file_size}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Check ctd.py to make sure data is being appropriatly processed and saved.\")\n",
    "\n",
    "\n",
    "\n",
    "def disgennet(url, filepath):\n",
    "    os.system(f'curl {url} -o {filepath}')\n",
    "    os.system(f'gunzip {filepath}')\n",
    "    # Validating data size \n",
    "    # CHECK NUMBER IN IF STATEMENT!!!!!\n",
    "    if os.path.getsize(filepath) < 5000:\n",
    "        print(f'Warning: {filepath} file may have not downloaded fully')\n",
    "\n",
    "\n",
    "def entrez_gene(url, filepath):\n",
    "    directory = os.getcwd()\n",
    "\n",
    "    # Downloading data\n",
    "    os.system(f'curl {url} -o {filepath}')\n",
    "    os.system(f'gunzip {filepath}')\n",
    "\n",
    "    # Validating data size \n",
    "    if os.path.getsize(filepath) < 373391360:\n",
    "        print(f'Warning: {filepath} file may have not downloaded fully')\n",
    "\n",
    "    # Running processing script\n",
    "    os.chdir(directory + '/processing_scripts')\n",
    "    os.system('python ncbigene.py')\n",
    "    os.chdir(directory)\n",
    "\n",
    "    # Validating processing script output\n",
    "    try:\n",
    "        file_size = os.path.getsize('data/ncbigene/protein_go_associations.csv')\n",
    "        print(f\"File Size of data/ncbigene/protein_go_associations.csv in Bytes is {file_size}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Check ncbigene.py to make sure data is being appropriatly processed and saved.\")\n",
    "\n",
    "\n",
    "def gene_ontology(url, filepath):\n",
    "    directory = os.getcwd()\n",
    "\n",
    "    # Downloading data\n",
    "    os.system(f'curl -L {url} -o {filepath}')\n",
    "    \n",
    "    # Validating data size \n",
    "    if os.path.getsize(filepath) < 15619420:\n",
    "        print(f'Warning: {filepath} file may have not downloaded fully')\n",
    "\n",
    "    # Runing processing script\n",
    "    os.chdir(directory + '/processing_scripts')\n",
    "    os.system('python go.py')\n",
    "    os.chdir(directory)\n",
    "\n",
    "    # Validating processing script output\n",
    "    try:\n",
    "        file_size = os.path.getsize('data/go/go_terms_relations.csv')\n",
    "        print(f\"File Size of data/go/go_terms_relations.csv in Bytes is {file_size}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Check go.py to make sure data is being appropriatly processed and saved.\")\n",
    "\n",
    "def hpo(url, filepath):\n",
    "    directory = os.getcwd()\n",
    "\n",
    "    # Downloading data\n",
    "    os.system(f'curl -L {url} -o {filepath}')\n",
    "\n",
    "    # Validating data size\n",
    "    if os.path.getsize(filepath) < 4907091:\n",
    "        print(f'Warning: {filepath} file may have not downloaded fully')\n",
    "\n",
    "    # Running processing script\n",
    "    os.chdir(directory + '/processing_scripts')\n",
    "    os.system('python hpo.py')\n",
    "    os.chdir(directory)\n",
    "\n",
    "    try:\n",
    "        file_size = os.path.getsize('data/hpo/hp_terms.csv')\n",
    "        print(f\"File Size of data/hpo/hp_terms.csv in Bytes is {file_size}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Check hpo.py to make sure data is being appropriatly processed and saved.\")\n",
    "\n",
    "def hpoa(url, filepath):\n",
    "    directory = os.getcwd()\n",
    "\n",
    "    # Downloading data\n",
    "    os.system(f'curl -L {url} -o {filepath}')\n",
    "\n",
    "    # Validating data size\n",
    "    if os.path.getsize(filepath) < 16406957:\n",
    "        print(f'Warning: {filepath} file may have not downloaded fully')\n",
    "\n",
    "    # Running processing script\n",
    "    os.chdir(directory + '/processing_scripts')\n",
    "    os.system('python hpoa.py')\n",
    "    os.chdir(directory)\n",
    "\n",
    "    try:\n",
    "        file_size = os.path.getsize('data/hpo/disease_phenotype_pos.csv')\n",
    "        print(f\"File Size of data/hpo/disease_phenotype_pos.csv in Bytes is {file_size}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Check hpoa.py to make sure data is being appropriatly processed and saved.\")\n",
    "\n",
    "def mondo(url, filepath):\n",
    "    directory = os.getcwd()\n",
    "\n",
    "    # Downloading data\n",
    "    os.system(f'curl -L {url} -o {filepath}')\n",
    "\n",
    "    # Validating data size\n",
    "    if os.path.getsize(filepath) < 23691202:\n",
    "        print(f'Warning: {filepath} file may have not downloaded fully')\n",
    "    \n",
    "    # Running processing script\n",
    "    os.chdir(directory + '/processing_scripts')\n",
    "    os.system('python mondo.py')\n",
    "    os.chdir(directory)\n",
    "\n",
    "    try:\n",
    "        file_size = os.path.getsize('data/mondo/mondo_terms.csv')\n",
    "        print(f\"File Size of data/mondo/mondo_terms.csv in Bytes is {file_size}.\")\n",
    "        file_size = os.path.getsize('data/mondo/mondo_parents.csv')\n",
    "        print(f\"File Size of data/mondo/mondo_parents.csv in Bytes is {file_size}.\")\n",
    "        file_size = os.path.getsize('data/mondo/mondo_references.csv')\n",
    "        print(f\"File Size of data/mondo/mondo_references.csv in Bytes is {file_size}.\")\n",
    "        file_size = os.path.getsize('data/mondo/mondo_subsets.csv')\n",
    "        print(f\"File Size of data/mondo/mondo_subsets.csv in Bytes is {file_size}.\")\n",
    "        file_size = os.path.getsize('data/mondo/mondo_definitions.csv')\n",
    "        print(f\"File Size of data/mondo/mondo_definitions.csv in Bytes is {file_size}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Check mondo.py to make sure data is being appropriatly processed and saved.\")\n",
    "\n",
    "\n",
    "def reactome(url, filepath):\n",
    "    directory = os.getcwd()\n",
    "    url = url.split(',')\n",
    "    filepath = filepath.split(',')\n",
    "\n",
    "    # Downloading data\n",
    "    os.system(f'curl {url[0]} -o {filepath[0]}')\n",
    "    os.system(f'curl {url[1]} -o {filepath[1]}')\n",
    "    os.system(f'curl {url[2]} -o {filepath[2]}')\n",
    "\n",
    "    # Validating data size\n",
    "    if os.path.getsize(filepath[0]) < 749101 or os.path.getsize(filepath[1]) < 300461 or os.path.getsize(filepath[2]) < 14762377:\n",
    "        print(f'Warning: {filepath} file may have not downloaded fully')\n",
    "\n",
    "    # Running processing script\n",
    "    os.chdir(directory + '/processing_scripts')\n",
    "    os.system('python reactome.py')\n",
    "    os.chdir(directory)\n",
    "\n",
    "    try:\n",
    "        file_size = os.path.getsize('data/reactome/reactome_ncbi.csv')\n",
    "        print(f\"File Size of data/reactome/reactome_ncbi.csv in Bytes is {file_size}.\")\n",
    "        file_size = os.path.getsize('data/reactome/reactome_relations.csv')\n",
    "        print(f\"File Size of data/reactome/reactome_relations.csv in Bytes is {file_size}.\")\n",
    "        file_size = os.path.getsize('data/reactome/reactome_terms.csv')\n",
    "        print(f\"File Size of data/reactome/reactome_terms.csv in Bytes is {file_size}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Check reactome.py to make sure data is being appropriatly processed and saved.\")\n",
    "\n",
    "def sider(url, filepath):\n",
    "    directory = os.getcwd()\n",
    "    url = url.split(',')\n",
    "    filepath = filepath.split(',')\n",
    "\n",
    "    # Downloading data\n",
    "    os.system(f'curl {url[0]} -o {filepath[0]}')\n",
    "    os.system(f'gunzip {filepath[0]}')\n",
    "    os.system(f'curl {url[1]} -o {filepath[1]}')\n",
    "\n",
    "    # Validating data size\n",
    "    if os.path.getsize(filepath[1]) < 16380: \n",
    "        print(f'Warning: {filepath} file may have not downloaded fully')\n",
    "\n",
    "    # Running processing script\n",
    "    os.chdir(directory + '/processing_scripts')\n",
    "    os.system('python sider.py')\n",
    "    os.chdir(directory)\n",
    "\n",
    "    try:\n",
    "        file_size = os.path.getsize('data/sider/sider.csv')\n",
    "        print(f\"File Size of data/sider/sider.csv in Bytes is {file_size}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Check sider.py to make sure data is being appropriatly processed and saved.\")\n",
    "\n",
    "\n",
    "def uberon(url, filepath):\n",
    "    directory = os.getcwd()\n",
    "\n",
    "    # Downloading data\n",
    "    os.system(f'curl -L {url} -o {filepath}')\n",
    "\n",
    "    # Validating data size\n",
    "    if os.path.getsize(filepath) < 171: \n",
    "        print(f'Warning: {filepath} file may have not downloaded fully')\n",
    "\n",
    "    # Running processing script\n",
    "    os.chdir(directory + '/processing_scripts')\n",
    "    os.system('python uberon.py')\n",
    "    os.chdir(directory)\n",
    "\n",
    "    try:\n",
    "        file_size = os.path.getsize('data/uberon/uberon_is_a.csv')\n",
    "        print(f\"File Size of data/uberon/uberon_is_a.csv in Bytes is {file_size}.\")\n",
    "        file_size = os.path.getsize('data/uberon/uberon_rels.csv')\n",
    "        print(f\"File Size of data/uberon/uberon_rels.csv in Bytes is {file_size}.\")\n",
    "        file_size = os.path.getsize('data/uberon/uberon_terms.csv')\n",
    "        print(f\"File Size of data/uberon/uberon_terms.csv in Bytes is {file_size}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Check uberon.py to make sure data is being appropriatly processed and saved.\")\n",
    "\n",
    "def drug_central(url, filepath):\n",
    "    os.system(f'curl {url} -o {filepath}')\n",
    "    os.system(f'gunzip {filepath}')\n",
    "\n",
    "    os.system('rm -rf data/drugcentral/db')\n",
    "    os.system('initdb -D data/drugcentral/db')\n",
    "    os.system('pg_ctl -D data/drugcentral/db -l logfile start')\n",
    "\n",
    "    os.system('createdb drugcentral')\n",
    "\n",
    "    # Replace /Users/vir... with path for drugcentral.dump.05102023.sql\n",
    "    os.system('psql drugcentral < /Users/vir/Testing/datasets/data/drugcentral/drugcentral.dump.05102023.sql')\n",
    "    os.system('psql -d drugcentral -c \\\"SELECT DISTINCT * FROM structures RIGHT JOIN (SELECT * FROM omop_relationship WHERE relationship_name IN (\\'indication\\', \\'contraindication\\', \\'off-label use\\')) AS drug_disease ON structures.id = drug_disease.struct_id;\\\" -P format=csv -o drug_disease.csv')\n",
    "    os.system('pg_ctl -D data/drugcentral/db stop')\n",
    "\n",
    "def drugbank():\n",
    "    # Drugbank files will need to be downloaded manually as Drugbank requires authentication before downloading data.\n",
    "    # Please download the Drugbank Complete Database, all carrier polypeptides, all enzyme polypeptides, target polypeptides,\n",
    "    # transporter polypeptides and all vocabulary to data/drugbank folder.\n",
    "\n",
    "    os.system('unzip data/drugbank/full_database.xml.zip -d data/drugbank')\n",
    "    os.system('rm data/drugbank/drugbank_all_full_database.xml.zip')\n",
    "\n",
    "    os.system('unzip data/drugbank/drugbank_all_carrier_polypeptide_ids.csv.zip -d data/drugbank/drugbank_all_carrier_polypeptide_ids.csv')\n",
    "    os.system('rm data/drugbank/drugbank_all_carrier_polypeptide_ids.csv.zip')\n",
    "\n",
    "    os.system('unzip data/drugbank/drugbank_all_enzyme_polypeptide_ids.csv.zip -d data/drugbank/drugbank_all_enzyme_polypeptide_ids.csv')\n",
    "    os.system('rm data/drugbank/drugbank_all_enzyme_polypeptide_ids.csv.zip')\n",
    "\n",
    "    os.system('unzip data/drugbank/drugbank_all_target_polypeptide_ids.csv.zip -d data/drugbank/drugbank_all_target_polypeptide_ids.csv')\n",
    "    os.system('rm data/drugbank/drugbank_all_target_polypeptide_ids.csv.zip')\n",
    "\n",
    "    os.system('unzip data/drugbank/drugbank_all_transporter_polypeptide_ids.csv.zip -d data/drugbank/drugbank_all_transporter_polypeptide_ids.csv')\n",
    "    os.system('rm data/drugbank/drugbank_all_transporter_polypeptide_ids.csv.zip')\n",
    "\n",
    "    os.chdir(directory + '/processing_scripts')\n",
    "    os.system('python drug_drug.py')\n",
    "    os.system('python drug_protein.py')\n",
    "    os.chdir(directory)\n",
    "\n",
    "\n",
    "def UMLS():\n",
    "    # UMLS files will need to be downloaded manually as UMLS requires authentication before downloading data.\n",
    "    # Please download the MRCONSO.RRF file to data/umls folder.\n",
    "\n",
    "    os.chdir(directory + '/processing_scripts')\n",
    "    os.system('python umls.py')\n",
    "    os.system('python map_umls_mondo.py')\n",
    "    os.chdir(directory)\n",
    "\n",
    "\n",
    "# initialize(directory)\n",
    "# gene_names(data_keys[0], data_dict.get(data_keys[0]))\n",
    "# bgee(data_keys[1], data_dict[data_keys[1]])\n",
    "# ctd(data_keys[2], data_dict[data_keys[2]])\n",
    "# disgennet(data_keys[3], data_dict[data_keys[3]])\n",
    "# entrez_gene(data_keys[4], data_dict[data_keys[4]])\n",
    "# gene_ontology(data_keys[5], data_dict[data_keys[5]])\n",
    "# hpo(data_keys[6], data_dict[data_keys[6]])\n",
    "# hpoa(data_keys[7], data_dict[data_keys[7]])\n",
    "# mondo(data_keys[8], data_dict[data_keys[8]])\n",
    "# reactome(data_keys[9], data_dict[data_keys[9]])\n",
    "# sider(data_keys[10], data_dict[data_keys[10]])\n",
    "# uberon(data_keys[11], data_dict[data_keys[11]])\n",
    "# drug_central(data_keys[12], data_dict[data_keys[12]])\n",
    "\n",
    "# run build_kg\n",
    "\n",
    "# get duplicate edges and see the ratio of duplicate edges to total edges\n",
    "# get total nodes and see ratio of total nodes with total edges\n",
    "\n",
    "\n",
    "# kg = pd.read_csv('/data/kg/auxillary/kg_raw.csv')\n",
    "# nodes = pd.read_csv('data/kg/auxillary/nodes.csv')\n",
    "# edges = pd.read_csv('data/kg/auxillary/edges.csv')\n",
    "\n",
    "\n",
    "\n",
    "def duplicate_edges(edges):\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    df['duplicate_edges'] = edges['x_index'].astype('string') + \"_\" + edges['y_index'].astype('string')\n",
    "    vc = df['duplicate_edges'].value_counts()\n",
    "\n",
    "    num_duplicate_edges = (vc[vc > 1]).shape[0]\n",
    "    if num_duplicate_edges > 3000:\n",
    "        print('Warning: there exist more than 3000 duplicate edges.')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
